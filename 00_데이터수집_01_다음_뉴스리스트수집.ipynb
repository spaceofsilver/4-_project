{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "import platform\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from selenium import webdriver as wd\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청 함수\n",
    "def getSource(site) :\n",
    "    \n",
    "    import bs4\n",
    "    import requests\n",
    "    \n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb Kit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers=header_info)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지에 있는 다음 뉴스 링크 수집 함수\n",
    "def getNewsLink(soup):\n",
    "\n",
    "    link_list = []\n",
    "\n",
    "    # li 태그 가져오기\n",
    "    a1 = soup.select('#clusterResultUL > li')\n",
    "    # print(len(a1))\n",
    "    \n",
    "    for a2 in a1:\n",
    "        \n",
    "        # div 태그 가져오기\n",
    "        a3 = a2.select_one('div.wrap_cont > div > div > a')\n",
    "        # print(a3)\n",
    "        \n",
    "        # 기사링크 \n",
    "        data1 = a3.attrs['href']\n",
    "        # print(data1)\n",
    "        \n",
    "        # 기사제목\n",
    "        data2 = a3.text.strip()\n",
    "        # print(data2)\n",
    "    \n",
    "        # span 태그 가져오기\n",
    "        a4 = a2.select_one('div.wrap_cont > div > span.f_nb.date')\n",
    "        a5 = a4.text.strip().split('|')\n",
    "        \n",
    "        # 날짜\n",
    "        data3 = a5[0]\n",
    "        \n",
    "        # 언론사\n",
    "        data4 = a5[1]\n",
    "        \n",
    "        # print(data1, data2, data3, data4)\n",
    "        \n",
    "        # 기사 링크 리스트에 저장\n",
    "        link_list.append(data1)\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df1 = pd.DataFrame(link_list)\n",
    "    display(df1)\n",
    "    \n",
    "#     FILENAME = 'chosun_link.csv'\n",
    "#     FILENAME = 'joongang_link.csv'\n",
    "#     FILENAME = 'donga_link.csv'\n",
    "#     FILENAME = 'jtbc_link.csv'\n",
    "    FILENAME = 'khan_link.csv'\n",
    "#    FILENAME = 'hani_link.csv'\n",
    "    if os.path.exists(FILENAME) == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False)\n",
    "    else:\n",
    "        # mode='a' : 기존 것 뒤에다 붙여줌\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False, header=False, mode='a')\n",
    "    \n",
    "        \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 페이지 존재 여부 확인하는 함수\n",
    "def getNextPage(site) :\n",
    "    \n",
    "    # url에서 p= 값 들고오기\n",
    "    p = site.split('&')[-1].split('=')[-1]\n",
    "    \n",
    "    # p값에 1 더해서 다음 페이지 url 만들기\n",
    "    nextPage = site[:-len(p)] + str(int(p)+1)\n",
    "    \n",
    "#     print(len(p))\n",
    "#     print(site)\n",
    "#     print(next_page)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 soup 가져오기\n",
    "    soup1 = getSource(site)\n",
    "    soup2 = getSource(nextPage)\n",
    "    # print(soup1)\n",
    "    # print(soup2)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 첫번째 a 태그에서 링크 가져오기\n",
    "    a1 = soup1.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    a2 = soup2.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    print(a1)\n",
    "    print(a2)\n",
    "   \n",
    " \n",
    "    # 두 링크가 같지 않으면 다음 페이지가 있다고 간주, 다음 페이지 return \n",
    "    if a1 != a2 :\n",
    "        return True\n",
    "    # 같으면 다음 페이지 없다고 간주, False return \n",
    "    else :\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 다음 뉴스 - 경향신문 : 2 페이지 수집 중\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://v.media.daum.net/v/20210407120423460?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://v.media.daum.net/v/20210407110533963?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://v.media.daum.net/v/20210407090237847?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://v.media.daum.net/v/20210407112819032?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://v.media.daum.net/v/20210407082127524?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030203266?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030022167?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://v.media.daum.net/v/20210407000204241?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030104197?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://v.media.daum.net/v/20210406220148873?f=o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0\n",
       "0  http://v.media.daum.net/v/20210407120423460?f=o\n",
       "1  http://v.media.daum.net/v/20210407110533963?f=o\n",
       "2  http://v.media.daum.net/v/20210407090237847?f=o\n",
       "3  http://v.media.daum.net/v/20210407112819032?f=o\n",
       "4  http://v.media.daum.net/v/20210407082127524?f=o\n",
       "5  http://v.media.daum.net/v/20210407030203266?f=o\n",
       "6  http://v.media.daum.net/v/20210407030022167?f=o\n",
       "7  http://v.media.daum.net/v/20210407000204241?f=o\n",
       "8  http://v.media.daum.net/v/20210407030104197?f=o\n",
       "9  http://v.media.daum.net/v/20210406220148873?f=o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://v.media.daum.net/v/20210407120423460?f=o\n",
      "http://v.media.daum.net/v/20210407120423460?f=o\n",
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "KEYWORD = urllib.parse.quote('보궐선거')\n",
    "# COLOPHON = '조선일보'\n",
    "# COLOPHON = '중앙일보'\n",
    "# COLOPHON = '동아일보'\n",
    "# COLOPHON = 'JTBC'\n",
    "COLOPHON = '경향신문'\n",
    "# COLOPHON = '한겨레'\n",
    "\n",
    "# dayStart = '20200910000000'\n",
    "# dayEnd = '20210310000000'\n",
    "# dayStart = '20210310000001'\n",
    "# dayEnd = '20210326000000'\n",
    "# 경향신문\n",
    "\n",
    "dayStart = '20210406215601'\n",
    "dayEnd   = '20210407200000'\n",
    "\n",
    "page = 1\n",
    "URL = 'https://search.daum.net/search?w=news&enc=utf8&cluster=y&cluster_page=1&'\n",
    "cp_dict = {'조선일보' : '16d4PV266g2j-N3GYq',\n",
    "           '중앙일보' : '16Elf9uX5H6T5xXvQV',\n",
    "           '동아일보' : '16bOiOx4gG2S18EPLj',\n",
    "           'JTBC'     : '16yZfDfR_rGcw5F-P0',\n",
    "           '경향신문' : '16akMkKFDu6n8GTzZr',\n",
    "           '한겨레' : '16nzyJHdH5ORpabfqG'}\n",
    "cpName = urllib.parse.quote(COLOPHON)\n",
    "cp = cp_dict[COLOPHON]\n",
    "\n",
    "while True :\n",
    "    time.sleep(1)\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "        \n",
    "    site = f'{URL}q={KEYWORD}&cpname={cpName}&cp={cp}&period=6m&sd={dayStart}&ed={dayEnd}&DA=PGD&p={page}'\n",
    "\n",
    "    print(f' 다음 뉴스 - {COLOPHON} : {page} 페이지 수집 중' )\n",
    "    \n",
    "    soup = getSource(site)\n",
    "    getNewsLink(soup) \n",
    "    chk = getNextPage(site)\n",
    "\n",
    "    if chk != False:\n",
    "        page = page + 1\n",
    "    else: \n",
    "        print('수집완료')\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
